"""
News Agent - ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì—ì´ì „íŠ¸
ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
"""

import os
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, asdict
from urllib.parse import quote


@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ëª¨ë¸"""
    title: str              # ì œëª©
    description: str        # ìš”ì•½
    url: str               # URL
    source: str            # ì¶œì²˜
    published_at: str      # ë°œí–‰ì¼ì‹œ
    author: Optional[str] = None  # ì €ì
    image_url: Optional[str] = None  # ì´ë¯¸ì§€ URL
    sentiment: Optional[float] = None  # ê°ì„± ì ìˆ˜ (-1 ~ 1)


class NewsAgent:
    """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì—ì´ì „íŠ¸"""
    
    def __init__(self, newsapi_key: Optional[str] = None, openai_key: Optional[str] = None):
        self.newsapi_key = newsapi_key or os.getenv("NEWSAPI_KEY", "")
        self.openai_key = openai_key or os.getenv("OPENAI_API_KEY", "")
        self.newsapi_url = "https://newsapi.org/v2"
        self.session = None
        
        # ë‰´ìŠ¤ ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜
        self.source_weights = {
            # êµ­ë‚´
            "ì—°í•©ë‰´ìŠ¤": 1.2,
            "í•œêµ­ê²½ì œ": 1.1,
            "ë§¤ì¼ê²½ì œ": 1.1,
            "ì¡°ì„ ë¹„ì¦ˆ": 1.0,
            "ì´ë°ì¼ë¦¬": 1.0,
            "ë¨¸ë‹ˆíˆ¬ë°ì´": 0.9,
            
            # í•´ì™¸
            "Reuters": 1.3,
            "Bloomberg": 1.3,
            "Financial Times": 1.2,
            "Wall Street Journal": 1.2,
            "CNBC": 1.1,
            "MarketWatch": 1.0,
            "Yahoo Finance": 0.9,
            "Seeking Alpha": 0.8
        }
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
            
    async def search_news(self,
                         query: str,
                         language: Optional[str] = None,
                         from_date: Optional[str] = None,
                         to_date: Optional[str] = None,
                         sort_by: str = "relevancy",
                         page_size: int = 20) -> Dict[str, Any]:
        """
        ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ì–´ (ì£¼ì‹ëª…, í‹°ì»¤ ë“±)
            language: ì–¸ì–´ ì½”ë“œ (ko, en)
            from_date: ì‹œì‘ì¼ (YYYY-MM-DD)
            to_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
            sort_by: ì •ë ¬ ë°©ì‹ (relevancy, popularity, publishedAt)
            page_size: í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜
        """
        if not self.newsapi_key:
            # API í‚¤ê°€ ì—†ìœ¼ë©´ RSS ê¸°ë°˜ ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ìš©
            print(f"[NEWS] Using RSS news for: {query}", flush=True)
            return await self._get_rss_news(query, language)
            
        # ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • - ìµœì í™”ëœ ê¸°ê°„ ì‚¬ìš©
        if not to_date:
            to_date = datetime.now().strftime("%Y-%m-%d")
        if not from_date:
            # PeriodConfigì—ì„œ ë‰´ìŠ¤ ìµœì  ê¸°ê°„ ê°€ì ¸ì˜¤ê¸°
            from config.period_config import PeriodConfig
            from_date = (datetime.now() - timedelta(days=PeriodConfig.NEWS_PERIOD_DAYS)).strftime("%Y-%m-%d")
            
        params = {
            "q": query,
            "from": from_date,
            "to": to_date,
            "sortBy": sort_by,
            "pageSize": page_size,
            "apiKey": self.newsapi_key
        }
        
        if language:
            params["language"] = language
            
        try:
            async with self.session.get(
                f"{self.newsapi_url}/everything",
                params=params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    articles = []
                    for item in data.get("articles", []):
                        article = NewsArticle(
                            title=item.get("title", ""),
                            description=item.get("description", ""),
                            url=item.get("url", ""),
                            source=item.get("source", {}).get("name", "Unknown"),
                            published_at=item.get("publishedAt", ""),
                            author=item.get("author"),
                            image_url=item.get("urlToImage")
                        )
                        articles.append(asdict(article))
                        
                    return {
                        "status": "success",
                        "total_results": data.get("totalResults", 0),
                        "articles": articles,
                        "data_source": "REAL_DATA"
                    }
                else:
                    return {
                        "status": "error",
                        "message": f"HTTP error: {response.status}"
                    }
                    
        except Exception as e:
            return {
                "status": "error",
                "message": f"Request failed: {str(e)}"
            }
            
    async def search_korean_news(self, 
                                company_name: str,
                                days: int = 7) -> Dict[str, Any]:
        """
        í•œêµ­ ê¸°ì—… ë‰´ìŠ¤ ê²€ìƒ‰ (êµ¬ê¸€ ë‰´ìŠ¤ RSS ì‚¬ìš©)
        
        Args:
            company_name: íšŒì‚¬ëª…
            days: ì¡°íšŒ ê¸°ê°„ (ì¼)
        """
        try:
            # êµ¬ê¸€ ë‰´ìŠ¤ RSSë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë‰´ìŠ¤ ê²€ìƒ‰
            import feedparser
            from urllib.parse import quote
            
            # í•œêµ­ ë‰´ìŠ¤ ê²€ìƒ‰ (êµ¬ê¸€ ë‰´ìŠ¤)
            search_query = quote(f"{company_name} ì£¼ì‹")
            google_news_url = f"https://news.google.com/rss/search?q={search_query}&hl=ko&gl=KR&ceid=KR:ko"
            
            feed = feedparser.parse(google_news_url)
            
            articles = []
            cutoff_date = datetime.now() - timedelta(days=days)
            
            for entry in feed.entries[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    pub_date = datetime.fromisoformat(entry.published.replace('Z', '+00:00').replace(' GMT', '+00:00'))
                except:
                    pub_date = datetime.now()
                
                if pub_date >= cutoff_date:
                    # ë‰´ìŠ¤ ì œëª©ì—ì„œ ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ
                    title = entry.title
                    summary = self._extract_key_info(title, company_name)
                    
                    articles.append({
                        "title": title,
                        "description": entry.summary if hasattr(entry, 'summary') else entry.title,
                        "url": entry.link,
                        "source": entry.source.href if hasattr(entry, 'source') else "Google News",
                        "published_at": pub_date.isoformat(),
                        "sentiment": None,
                        "key_info": summary
                    })
            
            if articles:
                return {
                    "status": "success",
                    "company": company_name,
                    "period_days": days,
                    "count": len(articles),
                    "articles": articles,
                    "data_source": "REAL_DATA",
                    "message": f"êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘"
                }
            else:
                # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ë§Œ ëª¨ì˜ ë°ì´í„° ë°˜í™˜
                return self._get_korean_mock_news(company_name)
                
        except Exception as e:
            print(f"Korean news search error: {str(e)}")
            # ì—ëŸ¬ ì‹œ ëª¨ì˜ ë°ì´í„° ë°˜í™˜
            return self._get_korean_mock_news(company_name)
    
    def _get_korean_mock_news(self, company_name: str) -> Dict[str, Any]:
        """í•œêµ­ ë‰´ìŠ¤ ëª¨ì˜ ë°ì´í„°"""
        mock_articles = [
            {
                "title": f"{company_name}, 3ë¶„ê¸° ì‹¤ì  ì‹œì¥ ì˜ˆìƒ ìƒíšŒ",
                "description": f"{company_name}ì´ 3ë¶„ê¸° ì‹¤ì ì—ì„œ ì‹œì¥ ì˜ˆìƒì„ í¬ê²Œ ì›ƒë„ëŠ” ì„±ê³¼ë¥¼ ê±°ë’€ë‹¤.",
                "url": "https://news.example.com/1",
                "source": "í•œêµ­ê²½ì œ",
                "published_at": datetime.now().isoformat(),
                "sentiment": 0.8
            },
            {
                "title": f"{company_name} ì‹ ì‚¬ì—… ì§„ì¶œ ë³¸ê²©í™”",
                "description": f"{company_name}ì´ AI ë¶„ì•¼ ì‹ ì‚¬ì—… ì§„ì¶œì„ ë³¸ê²©í™”í•œë‹¤ê³  ë°í˜”ë‹¤.",
                "url": "https://news.example.com/2",
                "source": "ë§¤ì¼ê²½ì œ",
                "published_at": (datetime.now() - timedelta(days=1)).isoformat(),
                "sentiment": 0.6
            }
        ]
        
        return {
            "status": "success",
            "company": company_name,
            "period_days": 7,
            "count": len(mock_articles),
            "articles": mock_articles,
            "data_source": "MOCK_DATA",
            "message": "âš ï¸ ëª¨ì˜ ë°ì´í„° - ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨"
        }
        
    async def search_financial_news(self,
                                   ticker: str,
                                   news_type: str = "all") -> Dict[str, Any]:
        """
        ì¬ë¬´ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            ticker: ì¢…ëª© í‹°ì»¤
            news_type: ë‰´ìŠ¤ ìœ í˜• (earnings, analysis, market)
        """
        # ì¬ë¬´ ë‰´ìŠ¤ì— íŠ¹í™”ëœ ê²€ìƒ‰
        query_map = {
            "earnings": f"{ticker} earnings report quarterly results",
            "analysis": f"{ticker} analyst rating price target",
            "market": f"{ticker} stock market trading volume",
            "all": ticker
        }
        
        query = query_map.get(news_type, ticker)
        
        # NewsAPI ë˜ëŠ” ë‹¤ë¥¸ ì¬ë¬´ ë‰´ìŠ¤ API ì‚¬ìš©
        result = await self.search_news(query, language="en")
        
        if result["status"] == "success":
            # ì¬ë¬´ ê´€ë ¨ í‚¤ì›Œë“œë¡œ í•„í„°ë§
            financial_keywords = [
                "earnings", "revenue", "profit", "quarterly", "analyst",
                "price target", "rating", "upgrade", "downgrade", "EPS"
            ]
            
            filtered_articles = []
            for article in result.get("articles", []):
                title_lower = article.get("title", "").lower()
                desc_lower = article.get("description", "").lower()
                
                # ì¬ë¬´ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ì„ íƒ
                if any(keyword in title_lower or keyword in desc_lower 
                      for keyword in financial_keywords):
                    filtered_articles.append(article)
                    
            result["articles"] = filtered_articles
            result["count"] = len(filtered_articles)
            # data_sourceëŠ” ì›ë³¸ ê²°ê³¼ì—ì„œ ìœ ì§€ë¨
            
        return result
        
    async def get_trending_news(self,
                               sector: Optional[str] = None,
                               limit: int = 10) -> Dict[str, Any]:
        """
        íŠ¸ë Œë”© ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            sector: ì„¹í„° (tech, finance, healthcare ë“±)
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        """
        # ì„¹í„°ë³„ ì£¼ìš” í‚¤ì›Œë“œ
        sector_keywords = {
            "tech": "technology stocks NASDAQ tech earnings",
            "finance": "banking financial services Fed interest rates",
            "healthcare": "pharmaceutical biotech FDA approval",
            "energy": "oil gas renewable energy crude prices",
            "retail": "consumer retail sales e-commerce"
        }
        
        query = sector_keywords.get(sector, "stock market trending")
        
        # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ íŠ¸ë Œë”© ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        result = await self.search_news(
            query=query,
            sort_by="publishedAt",
            page_size=limit
        )
        
        return result
        
    async def _get_mock_news(self, query: str) -> Dict[str, Any]:
        """API í‚¤ ì—†ì„ ë•Œ ëª¨ì˜ ë‰´ìŠ¤ ë°ì´í„° ë°˜í™˜"""
        mock_articles = [
            {
                "title": f"{query} Shows Strong Q3 Performance",
                "description": f"{query} reported better-than-expected earnings for Q3 2024",
                "url": "https://example.com/news/1",
                "source": "Financial Times",
                "date": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "published_at": datetime.now().isoformat(),
                "sentiment": 0.7
            },
            {
                "title": f"Analysts Upgrade {query} Price Target",
                "description": f"Major investment banks raise price targets for {query}",
                "url": "https://example.com/news/2",
                "source": "Reuters",
                "date": (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d %H:%M"),
                "published_at": (datetime.now() - timedelta(days=1)).isoformat(),
                "sentiment": 0.8
            },
            {
                "title": f"{query} Faces Regulatory Challenges",
                "description": f"Regulators scrutinize {query}'s market practices",
                "url": "https://example.com/news/3",
                "source": "Bloomberg",
                "date": (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d %H:%M"),
                "published_at": (datetime.now() - timedelta(days=2)).isoformat(),
                "sentiment": -0.3
            }
        ]
        
        return {
            "status": "success",
            "message": "âš ï¸ ëª¨ì˜ ë°ì´í„° - NewsAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ",
            "total_results": len(mock_articles),
            "count": len(mock_articles),
            "articles": mock_articles,
            "data_source": "MOCK_DATA"
        }
    
    async def _get_rss_news(self, query: str, language: str = "ko") -> Dict[str, Any]:
        """RSSë¥¼ í†µí•œ ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            import feedparser
            from urllib.parse import quote
            
            print(f"[RSS NEWS] Fetching news for: {query}, language: {language}", flush=True)
            
            if language == "ko" or any(ord(char) > 127 for char in query):
                # í•œêµ­ ë‰´ìŠ¤ - êµ¬ê¸€ ë‰´ìŠ¤
                search_query = quote(f"{query} ì£¼ì‹")
                rss_url = f"https://news.google.com/rss/search?q={search_query}&hl=ko&gl=KR&ceid=KR:ko"
            else:
                # ì˜ì–´ ë‰´ìŠ¤ - êµ¬ê¸€ ë‰´ìŠ¤
                search_query = quote(f"{query} stock")
                rss_url = f"https://news.google.com/rss/search?q={search_query}&hl=en&gl=US&ceid=US:en"
            
            print(f"[RSS NEWS] Fetching from URL: {rss_url}", flush=True)
            
            # RSS íŒŒì‹±
            feed = feedparser.parse(rss_url)
            
            articles = []
            for entry in feed.entries[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    # ë°œí–‰ì¼ íŒŒì‹±
                    pub_date = entry.published if hasattr(entry, 'published') else datetime.now().isoformat()
                    
                    # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                    title = entry.title if hasattr(entry, 'title') else "ì œëª© ì—†ìŒ"
                    description = entry.summary if hasattr(entry, 'summary') else title
                    link = entry.link if hasattr(entry, 'link') else ""
                    
                    # ì†ŒìŠ¤ ì¶”ì¶œ
                    source = "Google News"
                    if hasattr(entry, 'source') and hasattr(entry.source, 'title'):
                        source = entry.source.title
                    
                    articles.append({
                        "title": title,
                        "description": description,
                        "url": link,
                        "source": source,
                        "publishedAt": pub_date,
                        "sentiment": None,
                        "key_info": None
                    })
                    
                except Exception as article_error:
                    print(f"[RSS NEWS] Error parsing article: {article_error}", flush=True)
                    continue
                    
            print(f"[RSS NEWS] Found {len(articles)} articles", flush=True)
            
            if articles:
                return {
                    "status": "success",
                    "message": f"RSSì—ì„œ {len(articles)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ",
                    "total_results": len(articles),
                    "articles": articles,
                    "data_source": "REAL_DATA"
                }
            else:
                return {
                    "status": "error",
                    "message": "RSSì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "total_results": 0,
                    "articles": [],
                    "data_source": "REAL_DATA"
                }
                
        except Exception as e:
            print(f"[RSS NEWS] Error: {e}", flush=True)
            # RSS ì‹¤íŒ¨ ì‹œ ëª¨ì˜ ë°ì´í„° ë°˜í™˜
            return await self._get_mock_news(query)
        
    def calculate_news_sentiment_score(self, articles: List[Dict]) -> float:
        """
        ë‰´ìŠ¤ ëª©ë¡ì˜ ì „ì²´ ê°ì„± ì ìˆ˜ ê³„ì‚°
        
        Args:
            articles: ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡
            
        Returns:
            ì „ì²´ ê°ì„± ì ìˆ˜ (-1 ~ 1)
        """
        if not articles:
            return 0.0
            
        total_score = 0.0
        total_weight = 0.0
        
        for article in articles:
            # ì¶œì²˜ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            source = article.get("source", "Unknown")
            weight = self.source_weights.get(source, 0.5)
            
            # ê°œë³„ ê¸°ì‚¬ ê°ì„± ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ê°ì„± ë¶„ì„ ëª¨ë¸ ì‚¬ìš©)
            sentiment = article.get("sentiment", 0.0)
            
            total_score += sentiment * weight
            total_weight += weight
            
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def _extract_key_info(self, title: str, company_name: str) -> str:
        """
        ë‰´ìŠ¤ ì œëª©ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        
        ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì •ë³´:
        - ëª©í‘œê°€ ë³€ê²½
        - ì‹¤ì  ë°œí‘œ
        - ì£¼ìš” ê³„ì•½/íŒŒíŠ¸ë„ˆì‹­
        - ê·œì œ/ì •ì±… ë³€í™”
        - ê²½ì˜ì§„ ë³€ê²½
        """
        key_info = []
        
        # ëª©í‘œê°€ ê´€ë ¨
        if "ëª©í‘œê°€" in title or "price target" in title.lower():
            if "ìƒí–¥" in title or "raise" in title.lower() or "upgrade" in title.lower():
                key_info.append("ğŸ“ˆ ëª©í‘œê°€ ìƒí–¥")
            elif "í•˜í–¥" in title or "lower" in title.lower() or "downgrade" in title.lower():
                key_info.append("ğŸ“‰ ëª©í‘œê°€ í•˜í–¥")
            else:
                key_info.append("ğŸ¯ ëª©í‘œê°€ ì¡°ì •")
                
        # ì‹¤ì  ê´€ë ¨
        if "ì‹¤ì " in title or "earnings" in title.lower() or "profit" in title.lower():
            if "í˜¸ì¡°" in title or "ìƒìŠ¹" in title or "beat" in title.lower():
                key_info.append("ğŸ’° ì‹¤ì  í˜¸ì¡°")
            elif "ë¶€ì§„" in title or "í•˜ë½" in title or "miss" in title.lower():
                key_info.append("âš ï¸ ì‹¤ì  ë¶€ì§„")
                
        # ì£¼ê°€ ì›€ì§ì„
        if any(keyword in title for keyword in ["ê¸‰ë“±", "ê¸‰ë½", "ìƒìŠ¹", "í•˜ë½", "surge", "plunge", "rise", "fall"]):
            if any(keyword in title for keyword in ["ê¸‰ë“±", "ìƒìŠ¹", "surge", "rise"]):
                key_info.append("ğŸ“Š ì£¼ê°€ ìƒìŠ¹")
            else:
                key_info.append("ğŸ“‰ ì£¼ê°€ í•˜ë½")
                
        # ì‚¬ì—…/ê³„ì•½ ê´€ë ¨
        if any(keyword in title for keyword in ["ê³„ì•½", "ìˆ˜ì£¼", "íŒŒíŠ¸ë„ˆì‹­", "ì¸ìˆ˜", "í•©ë³‘", "contract", "partnership", "acquisition"]):
            key_info.append("ğŸ¤ ì£¼ìš” ê³„ì•½/íŒŒíŠ¸ë„ˆì‹­")
            
        # ê¸°ìˆ /ì œí’ˆ ê´€ë ¨
        if any(keyword in title for keyword in ["ì‹ ì œí’ˆ", "ì¶œì‹œ", "ê°œë°œ", "í˜ì‹ ", "launch", "develop", "innovation"]):
            key_info.append("ğŸš€ ì‹ ì œí’ˆ/ê¸°ìˆ  ê°œë°œ")
            
        # ê·œì œ/ì •ì±…
        if any(keyword in title for keyword in ["ê·œì œ", "ì •ì±…", "ë²•ì•ˆ", "ì œì¬", "regulation", "policy", "sanction"]):
            key_info.append("âš–ï¸ ê·œì œ/ì •ì±… ì´ìŠˆ")
            
        # ë°°ë‹¹/ìì‚¬ì£¼
        if any(keyword in title for keyword in ["ë°°ë‹¹", "ìì‚¬ì£¼", "dividend", "buyback"]):
            key_info.append("ğŸ’µ ì£¼ì£¼í™˜ì› ì •ì±…")
        
        return " | ".join(key_info) if key_info else ""


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_news_agent():
    """News Agent í…ŒìŠ¤íŠ¸"""
    print("=== News Agent í…ŒìŠ¤íŠ¸ ===\\n")
    
    async with NewsAgent() as agent:
        # 1. ì¼ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰
        print("1. Apple ë‰´ìŠ¤ ê²€ìƒ‰")
        result = await agent.search_news("Apple stock", language="en", page_size=5)
        if result["status"] == "success":
            print(f"ì „ì²´ ê²°ê³¼: {result.get('total_results')}ê±´")
            for article in result.get("articles", [])[:3]:
                print(f"\\nì œëª©: {article['title']}")
                print(f"ì¶œì²˜: {article['source']}")
                print(f"ë°œí–‰ì¼: {article['published_at']}")
                if article.get('description'):
                    print(f"ìš”ì•½: {article['description'][:100]}...")
                    
        print("\\n" + "-" * 50 + "\\n")
        
        # 2. í•œêµ­ ê¸°ì—… ë‰´ìŠ¤
        print("2. ì‚¼ì„±ì „ì ë‰´ìŠ¤ ê²€ìƒ‰")
        result = await agent.search_korean_news("ì‚¼ì„±ì „ì", days=7)
        if result["status"] == "success":
            print(f"ê²€ìƒ‰ ê¸°ê°„: ìµœê·¼ {result['period_days']}ì¼")
            print(f"ê²€ìƒ‰ ê²°ê³¼: {result['count']}ê±´")
            for article in result.get("articles", []):
                print(f"- [{article['source']}] {article['title']}")
                print(f"  ê°ì„± ì ìˆ˜: {article.get('sentiment', 0)}")
                
        print("\\n" + "-" * 50 + "\\n")
        
        # 3. ì¬ë¬´ ë‰´ìŠ¤ ê²€ìƒ‰
        print("3. Tesla ì¬ë¬´ ë‰´ìŠ¤")
        result = await agent.search_financial_news("TSLA", news_type="earnings")
        if result["status"] == "success":
            print(f"ì¬ë¬´ ê´€ë ¨ ë‰´ìŠ¤: {result.get('count', len(result.get('articles', [])))}ê±´")
            for article in result.get("articles", [])[:2]:
                print(f"- {article['title']}")
                
        print("\\n" + "-" * 50 + "\\n")
        
        # 4. ê°ì„± ì ìˆ˜ ê³„ì‚°
        print("4. ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ ê³„ì‚°")
        if result.get("articles"):
            sentiment_score = agent.calculate_news_sentiment_score(result["articles"])
            print(f"ì „ì²´ ê°ì„± ì ìˆ˜: {sentiment_score:.2f} (-1 ~ 1)")
            if sentiment_score > 0.3:
                print("â¡ï¸ ê¸ì •ì ì¸ ë‰´ìŠ¤ê°€ ìš°ì„¸í•©ë‹ˆë‹¤")
            elif sentiment_score < -0.3:
                print("â¡ï¸ ë¶€ì •ì ì¸ ë‰´ìŠ¤ê°€ ìš°ì„¸í•©ë‹ˆë‹¤")
            else:
                print("â¡ï¸ ì¤‘ë¦½ì ì¸ ë‰´ìŠ¤ê°€ ìš°ì„¸í•©ë‹ˆë‹¤")
                

if __name__ == "__main__":
    asyncio.run(test_news_agent())